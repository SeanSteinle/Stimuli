{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprisal Models\n",
    "**Brief**:<br>\n",
    "This will be the main file for all of our model loading, data organizing, searching, etc.<br><br>\n",
    "**Sections**:\n",
    "1. [Methodology And Instructions For Running](#pre)\n",
    "2. [Starting A StanfordCoreNLP Server](#1)\n",
    "    - [Background](#1_a)\n",
    "    - [How To Run The Server](#1_b)\n",
    "    - [Resources](#1_c)\n",
    "    - [Code](#1_d)\n",
    "3. [Applying Models](#2)\n",
    "___\n",
    "<a id='pre'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology And Instructions For Running\n",
    "In this section, we'll summarize and catch up to where we are now.<br>\n",
    "**Installations**:\n",
    "There are two quick installations that will catch you up with the workflow of using Python.\n",
    "1. Git Bash (https://git-scm.com/downloads)\n",
    "    - This is a modified version of the command line (or terminal in MacOS) that enables the user to download GitHub repositories to their local machine, contribute to these repositories, and then publish their results.\n",
    "    - I'm not going to do a big whole thing on Git here, but I will have you:\n",
    "        - **clone** our repository to your machine, \n",
    "        - **add** a few files, \n",
    "        - **commit** your changes, \n",
    "        - and finally submit a **pull request**.\n",
    "    - Why go through this hassle? Now, you can work with our code on your machine in Juypter, as opposed to having look at it on the GitHub interface!\n",
    "2. Anaconda (https://www.anaconda.com/distribution/)\n",
    "    - This is an extremely popular platform for data science work. I think you might already have it, so I won't go into much detail. Basically we'll be using this so we can access Juypter Notebooks (like this one!).\n",
    "<br><br>\n",
    "\n",
    "**A Road Map to Tag and Word Level Probability**:\n",
    "It's a long road from a raw sentence to a number than represents the likelihood of a given word or tag. Let's see how we can get there.\n",
    "1. Sentence Tagging\n",
    "    - sentences -> list of tuples, each tuple has word and tag\n",
    "    - sometimes this step will be consumed by the subsequent parsing step. for example, the Stanford parser accepts raw sentences as input, does the tagging, and then does the parsing.\n",
    "        - *parser.raw_parse('The King of France is Bald.')* does this\n",
    "2. Sentence Parsing\n",
    "    - list of tags -> a tree\n",
    "    - where we are at. need to talk to Na Rae\n",
    "\n",
    "<a id='#1'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting A StanfordCoreNLP Server\n",
    "<a id='1_a'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**:<br>\n",
    "We've spent a lot of time looking at the StanfordCoreNLP software, and we ultimately decided we want to use both the parser and tagger (really, the parser automatically implements the tagger, but we'll see later). While the software is very useful, it's written in Java--which is not quite as nice to play with as Python for a number of reasons (lacks NLP libraries, lower level language, etc). The problem then is finding a way to use a Java program like it's a Python program.<br><br>\n",
    "The two main options I looked into was a traditional import and a private server. One route of solving our problem is to use a traditional \"wrapper\" library/program. This program is essentially a translator between Java and Python. Unfortunately, the Stanford team itself doesn't actually make these wrappers (they would have to make them for a *lot* of languages). The existing wrappers--specifically the <u>stanfordcorenlp</u> library--weren't available through Anaconda (the platform through which we are running this exciting program right now), so I went another route.<br><br>\n",
    "The direction I chose was to host a server that runs the out-of-the-box Java program, and to access it through a Python API. This involves a small amount of command line setup, but it saves the trouble of changing environment variables or using directories in Python.<br><br>\n",
    "<a id='1_b'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How To Run A Server**<br>\n",
    "*Note*: Huge thanks to Khalid Alnajjar, linked his guide in resources.\n",
    "I've never actually hosted any sort of server before, so here's a quick summary:\n",
    "1. download and extract the CoreNLP somewhere.\n",
    "2. on the command line, cd into that directory\n",
    "    - *stanford-corenlp-full-2018-10-05* should be the folder\n",
    "3. run this command to host the server:\n",
    "    - java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"tokenize,ssplit,pos,lemma,parse,sentiment\" -port 9000 -timeout 30000 \n",
    "4. pick up [here](#1_d)\n",
    "<a id='1_c'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**:<br>\n",
    "1. documentation for the parser: http://www.nltk.org/_modules/nltk/parse/stanford.html\n",
    "2. for more on running the server: https://www.khalidalnajjar.com/setup-use-stanford-corenlp-server-python/\n",
    "3. StanfordCoreNLP's GitHub page: https://github.com/stanfordnlp/CoreNLP\n",
    "4. stanfordcorenlp wrapper library: https://pypi.org/project/stanfordcorenlp/\n",
    "\n",
    "<a id='1_d'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk import StanfordPOSTagger\n",
    "from nltk.parse import stanford\n",
    "from nltk.parse import CoreNLPParser\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#builds parser\n",
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['The']), Tree('NNP', ['King'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('NNP', ['France'])])])]), Tree('VP', [Tree('VBZ', ['is']), Tree('ADJP', [Tree('JJ', ['Bald'])])]), Tree('.', ['.'])])])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['_OUTPUT_FORMAT',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_params',\n",
       " 'api_call',\n",
       " 'encoding',\n",
       " 'evaluate',\n",
       " 'grammar',\n",
       " 'make_tree',\n",
       " 'parse',\n",
       " 'parse_all',\n",
       " 'parse_one',\n",
       " 'parse_sents',\n",
       " 'parse_text',\n",
       " 'parser_annotator',\n",
       " 'raw_parse',\n",
       " 'raw_parse_sents',\n",
       " 'raw_tag_sents',\n",
       " 'session',\n",
       " 'span_tokenize',\n",
       " 'span_tokenize_sents',\n",
       " 'tag',\n",
       " 'tag_sents',\n",
       " 'tagtype',\n",
       " 'tokenize',\n",
       " 'tokenize_sents',\n",
       " 'url']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing parser on sentence\n",
    "list(parser.raw_parse('The King of France is Bald.'))\n",
    "#if you want to see the list of commands\n",
    "dir(parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: How to make sense of this object, especially with NLTK's trees? Here's some help: https://stackoverflow.com/questions/26210567/get-entities-from-nltk-tree-result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.parse.corenlp.CoreNLPParser"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "list_iterator"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parser)\n",
    "type(parser.raw_parse('The King of France is Bald.'))\n",
    "parsed_sent = parser.raw_parse('The King of France is Bald.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7190c37c2e09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparsed_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_sent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36mfromstring\u001b[1;34m(cls, s, brackets, read_node, read_leaf, node_pattern, leaf_pattern, remove_empty_top_bracketing)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;31m# Walk through each token, updating a stack of trees.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# list of (node, children) tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken_re\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[1;31m# Beginning of a tree/subtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "parsed_tree = nltk.Tree.fromstring(parsed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.grammar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
