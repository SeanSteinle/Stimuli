{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading excel doc into a Panda dataframe. this makes accessing the and mutating the code a lot easier to work with.\n",
    "\n",
    "(NaN means no value in the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Issue Issue Info Source   Category       Date First 2  Character Count  \\\n",
      "0      NaN        NaN   Kyra    Science 2020-09-18     Yes               10   \n",
      "1      NaN        NaN   Kyra    Opinion 2020-10-16     Yes               11   \n",
      "2      NaN        NaN  Mushi    Science 2020-11-26     Yes               12   \n",
      "3      NaN        NaN  Mushi   Business 2019-07-19     Yes               12   \n",
      "4      NaN        NaN  Mushi       U.S. 2019-01-30     Yes                9   \n",
      "..     ...        ...    ...        ...        ...     ...              ...   \n",
      "112    NaN        NaN   Kyra      World 2020-01-18     Yes               11   \n",
      "113    NaN        NaN   Kyra    Opinion 2020-01-18     Yes               12   \n",
      "114    NaN        NaN   Kyra    Opinion 2020-06-16     Yes               11   \n",
      "115    NaN        NaN   Kyra    Opinion 2020-06-12     Yes                9   \n",
      "116    NaN        NaN   Kyra  Parenting 2020-12-09     Yes                7   \n",
      "\n",
      "      Longest word  WC_Item  WC_S1  ...  Check - does WC_S1+WC_S2=WC_Item  \\\n",
      "0      reinforced.       27     17  ...                                OK   \n",
      "1     information.       30     12  ...                                OK   \n",
      "2    partnerships.       23     11  ...                                OK   \n",
      "3     presidential       30     19  ...                                OK   \n",
      "4        lawmakers       16     10  ...                                OK   \n",
      "..             ...      ...    ...  ...                               ...   \n",
      "112   storefronts.       35     11  ...                                OK   \n",
      "113   relationship       26     11  ...                                OK   \n",
      "114    experienced       34     11  ...                                OK   \n",
      "115      reflected       19      7  ...                                OK   \n",
      "116       breathe.       15      8  ...                                OK   \n",
      "\n",
      "                                                    S1  \\\n",
      "0    Men have a far greater appetite for sex and ar...   \n",
      "1    In the right hands, location data can be a for...   \n",
      "2    Male field crickets perform mating songs and d...   \n",
      "3    Paying for college seems out of reach for many...   \n",
      "4    Very few issues can bring together lawmakers o...   \n",
      "..                                                 ...   \n",
      "112  For the rural firefighters of Australia, no do...   \n",
      "113  Our relationship to privacy is inseparable fro...   \n",
      "114  Businesses do themselves a disservice by shyin...   \n",
      "115                 Earth is warming, and we know why.   \n",
      "116       Nobody ever really tells you how to breathe.   \n",
      "\n",
      "                                                    S2  \\\n",
      "0    This is the timeworn stereotype that science h...   \n",
      "1    It can provide publishers and app developers w...   \n",
      "2    Female Japanese macaque monkeys pair off into ...   \n",
      "3    Several Democratic presidential candidates and...   \n",
      "4                       Animal cruelty is one of them.   \n",
      "..                                                 ...   \n",
      "112  They have long been known to canvass for dolla...   \n",
      "113  If you trust someone, you may be more willing ...   \n",
      "114  Rather than worrying about only the potential ...   \n",
      "115  Light is reflected and absorbed by clouds, air...   \n",
      "116                        You just know how to do it.   \n",
      "\n",
      "                                  CompQ - True Version  \\\n",
      "0    Science has reinforced the stereotype that men...   \n",
      "1    Location data can be used to generate advertis...   \n",
      "2    The animals have relationships with their same...   \n",
      "3    Free college has been endorsed by several Demo...   \n",
      "4    Lawmakers are united on the issue of animal cr...   \n",
      "..                                                 ...   \n",
      "112  Rural Australian firefighters are known to can...   \n",
      "113                                                NaN   \n",
      "114  Companies do not realize the potential benefit...   \n",
      "115  The Earth is warming due to the constant refle...   \n",
      "116            Nobody has to be taught how to breathe.   \n",
      "\n",
      "                                 CompQ - False Version CompR  \\\n",
      "0    Science has reinforced a stereotype that women...   0.0   \n",
      "1    Location data cannot be used for anything that...   1.0   \n",
      "2    The monkeys perform mating songs and dances fo...   NaN   \n",
      "3    Free college has been endorsed by several Repu...   1.0   \n",
      "4    Lawmakers often disagree on the issue of anima...   NaN   \n",
      "..                                                 ...   ...   \n",
      "112                                                NaN   NaN   \n",
      "113  Our relationships with privacy and trust are s...   NaN   \n",
      "114  Companies worry about the cost of hiring young...   NaN   \n",
      "115               Light cannot be absorbed by the air.   NaN   \n",
      "116                  People must learn how to breathe.   NaN   \n",
      "\n",
      "                                           CompQ Notes  \\\n",
      "0                                                  NaN   \n",
      "1                                                  NaN   \n",
      "2    Difficult to incorporate both sentences into t...   \n",
      "3                                         Too obvious?   \n",
      "4                                                  NaN   \n",
      "..                                                 ...   \n",
      "112                                                NaN   \n",
      "113                                                NaN   \n",
      "114                                                NaN   \n",
      "115                                                NaN   \n",
      "116                                                NaN   \n",
      "\n",
      "                                    CompQ thoughts-Max ORDER  \\\n",
      "0                                                 Good   2.0   \n",
      "1                                                 Good   9.0   \n",
      "2         Needs COMPQ, previously filtered incorrectly  14.0   \n",
      "3    Both are true, just only 1 is mentioned by the...  19.0   \n",
      "4         Needs COMPQ, previously filtered incorrectly  20.0   \n",
      "..                                                 ...   ...   \n",
      "112                              Needs CompQ, new item   NaN   \n",
      "113                              Needs CompQ, new item   NaN   \n",
      "114                              Needs CompQ, new item   NaN   \n",
      "115                              Needs CompQ, new item   NaN   \n",
      "116                              Needs CompQ, new item   NaN   \n",
      "\n",
      "     Cassie's Comments  \n",
      "0                 good  \n",
      "1                 good  \n",
      "2                  NaN  \n",
      "3                 good  \n",
      "4                  NaN  \n",
      "..                 ...  \n",
      "112                NaN  \n",
      "113                NaN  \n",
      "114                NaN  \n",
      "115                NaN  \n",
      "116                NaN  \n",
      "\n",
      "[117 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "stimuli_sheet = pd.read_excel('Stimuli.xlsx')\n",
    "print(stimuli_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first and second sentence column (S1 and S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentences: \n",
      " 0      Men have a far greater appetite for sex and ar...\n",
      "1      In the right hands, location data can be a for...\n",
      "2      Male field crickets perform mating songs and d...\n",
      "3      Paying for college seems out of reach for many...\n",
      "4      Very few issues can bring together lawmakers o...\n",
      "                             ...                        \n",
      "112    For the rural firefighters of Australia, no do...\n",
      "113    Our relationship to privacy is inseparable fro...\n",
      "114    Businesses do themselves a disservice by shyin...\n",
      "115                   Earth is warming, and we know why.\n",
      "116         Nobody ever really tells you how to breathe.\n",
      "Name: S1, Length: 117, dtype: object \n",
      "\n",
      "Second Sentences: \n",
      " 0      This is the timeworn stereotype that science h...\n",
      "1      It can provide publishers and app developers w...\n",
      "2      Female Japanese macaque monkeys pair off into ...\n",
      "3      Several Democratic presidential candidates and...\n",
      "4                         Animal cruelty is one of them.\n",
      "                             ...                        \n",
      "112    They have long been known to canvass for dolla...\n",
      "113    If you trust someone, you may be more willing ...\n",
      "114    Rather than worrying about only the potential ...\n",
      "115    Light is reflected and absorbed by clouds, air...\n",
      "116                          You just know how to do it.\n",
      "Name: S2, Length: 117, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"First Sentences: \\n\", stimuli_sheet.S1, \"\\n\\nSecond Sentences: \\n\",stimuli_sheet.S2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is opening up the tagger object I hastily trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_load = open(\"nltkTagger.pickle\",\"rb\")\n",
    "tagger = pickle.load(pickle_load)\n",
    "pickle_load.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaded a few sentences into a list, tokenized them with NLTK, tagged them with my tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Men', 'NNS'), ('have', 'HV'), ('a', 'AT'), ('far', 'JJ'), ('greater', 'JJR'), ('appetite', 'NN'), ('for', 'IN'), ('sex', 'NN'), ('and', 'CC'), ('are', 'BER'), ('more', 'QL'), ('attracted', 'VBN'), ('to', 'TO'), ('pornography', 'NN'), ('than', 'CS'), ('women', 'NNS'), ('are', 'BER'), ('.', '.')]\n",
      "[('In', 'IN'), ('the', 'AT'), ('right', 'JJ'), ('hands', 'NNS'), (',', ','), ('location', 'NN'), ('data', 'NN'), ('can', 'MD'), ('be', 'BE'), ('a', 'AT'), ('force', 'NN'), ('for', 'IN'), ('good', 'JJ'), ('.', '.')]\n",
      "[('Male', 'NP'), ('field', 'NN'), ('crickets', 'NNS'), ('perform', 'VB'), ('mating', 'VBG'), ('songs', 'NNS'), ('and', 'CC'), ('dances', 'NNS'), ('for', 'IN'), ('each', 'DT'), ('other', 'AP'), ('.', '.')]\n",
      "[('Paying', 'VBG'), ('for', 'IN'), ('college', 'NN'), ('seems', 'VBZ'), ('out', 'RP'), ('of', 'IN'), ('reach', 'NN'), ('for', 'IN'), ('many', 'AP'), ('Americans', 'NPS'), (',', ','), ('so', 'QL'), ('the', 'AT'), ('idea', 'NN'), ('of', 'IN'), ('free', 'JJ'), ('college', 'NN'), ('has', 'HVZ'), ('broad', 'JJ'), ('appeal', 'NN'), ('.', '.')]\n",
      "[('Very', 'QL'), ('few', 'AP'), ('issues', 'NNS'), ('can', 'MD'), ('bring', 'VB'), ('together', 'RB'), ('lawmakers', 'NNS'), ('of', 'IN'), ('both', 'ABX'), ('parties', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "first_sents = stimuli_sheet['S1']\n",
    "play_sents = []\n",
    "play_sents = first_sents[:5]\n",
    "\n",
    "play_tokenized = []\n",
    "\n",
    "for sent in play_sents:\n",
    "    play_tokenized.append(nltk.word_tokenize(sent))\n",
    "for sent in play_tokenized:\n",
    "    print(tagger.tag(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets tag the whole thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ten', 'CD'), ('years', 'NNS'), ('ago', 'RB'), (',', ','), ('checklists', 'NNS'), ('for', 'IN'), ('surgeons', 'NNS'), ('were', 'BED'), ('all', 'ABN'), ('the', 'AT'), ('rage', 'NN'), ('.', '.')]\n",
      "\n",
      "[('Inspired', 'VBD'), ('by', 'IN'), ('the', 'AT'), ('preflight', 'JJ'), ('routines', 'NNS'), ('of', 'IN'), ('airline', 'NN'), ('pilots', 'NNS'), (',', ','), ('surgical', 'JJ'), ('checklists', 'NNS'), ('were', 'BED'), ('shown', 'VBN'), ('to', 'TO'), ('prevent', 'VB'), ('tragic', 'JJ'), ('errors', 'NNS'), (',', ','), ('reduce', 'VB'), ('infections', 'NNS'), ('and', 'CC'), ('save', 'VB'), ('lives', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "s1_tokens = [] #list of word tokens from the first sentences\n",
    "s1_tagged = [] #list of tagged word tokens from the first sentences\n",
    "\n",
    "for sent in first_sents: #filling s1_tokens\n",
    "    s1_tokens.append(nltk.word_tokenize(sent))\n",
    "for sent in s1_tokens: #filling s1_tagged\n",
    "    s1_tagged.append(tagger.tag(sent))\n",
    "print(s1_tagged[101]) #sample output\n",
    "\n",
    "print()\n",
    "#doing the exact same thing for the second sentences\n",
    "second_sents = stimuli_sheet['S2']\n",
    "s2_tokens = [] #list of word tokens from the second sentences\n",
    "s2_tagged = [] #list of tagged word tokens from the second sentences\n",
    "\n",
    "for sent in second_sents: #filling s2_tokens\n",
    "    s2_tokens.append(nltk.word_tokenize(sent))\n",
    "for sent in s2_tokens: #filling s2_tagged\n",
    "    s2_tagged.append(tagger.tag(sent))\n",
    "print(s2_tagged[101]) #sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
