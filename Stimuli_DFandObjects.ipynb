{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all:\n",
    "\n",
    "this is my first attempt to do some data cleaning and preparation for our stimuli data in excel. bear with me, still new to a lot of the packages I'm using, but it ends up working pretty well.\n",
    "\n",
    "statistically relevant columns:\n",
    "(lets fill this out)\n",
    "\n",
    "resources:\n",
    "Penn Treebank Tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading excel doc into a Panda dataframe. this makes accessing the and mutating the code a lot easier to work with.\n",
    "\n",
    "(NaN means no value in the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_sheet = pd.read_excel('Stimuli.xlsx')\n",
    "print(stimuli_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first and second sentence column (S1 and S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First Sentences: \\n\", stimuli_sheet.S1, \"\\n\\nSecond Sentences: \\n\",stimuli_sheet.S2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is opening up the tagger object I hastily trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_load = open(\"Stanford_POS.pkl\",\"rb\")\n",
    "tagger = pickle.load(pickle_load)\n",
    "pickle_load.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaded a few sentences into a list, tokenized them with NLTK, tagged them with my tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sents = stimuli_sheet['S1']\n",
    "play_sents = []\n",
    "play_sents = first_sents[:5]\n",
    "\n",
    "play_tokenized = []\n",
    "\n",
    "for sent in play_sents:\n",
    "    play_tokenized.append(nltk.word_tokenize(sent))\n",
    "for sent in play_tokenized:\n",
    "    print(tagger.tag(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets tag the whole thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_tokens = [] #list of word tokens from the first sentences\n",
    "s1_tagged = [] #list of tagged word tokens from the first sentences\n",
    "\n",
    "for sent in first_sents: #filling s1_tokens\n",
    "    s1_tokens.append(nltk.word_tokenize(sent))\n",
    "for sent in s1_tokens: #filling s1_tagged\n",
    "    s1_tagged.append(tagger.tag(sent))\n",
    "print(s1_tagged[101]) #sample output\n",
    "\n",
    "print()\n",
    "#doing the exact same thing for the second sentences\n",
    "second_sents = stimuli_sheet['S2']\n",
    "s2_tokens = [] #list of word tokens from the second sentences\n",
    "s2_tagged = [] #list of tagged word tokens from the second sentences\n",
    "\n",
    "for sent in second_sents: #filling s2_tokens\n",
    "    s2_tokens.append(nltk.word_tokenize(sent))\n",
    "for sent in s2_tokens: #filling s2_tagged\n",
    "    s2_tagged.append(tagger.tag(sent))\n",
    "print(s2_tagged[101]) #sample output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets get tags only (for syntactic surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_tags = [] #this will hold all of the tags\n",
    "for sent in s1_tagged: #filling s1_tags\n",
    "    s1_tags.append([(y) for (x,y) in sent])\n",
    "print(s1_tags[94]) #sample\n",
    "\n",
    "#same for s2\n",
    "s2_tags = [] #this will hold all of the tags\n",
    "for sent in s2_tagged: #filling s1_tags\n",
    "    s2_tags.append([(y) for (x,y) in sent])\n",
    "print(s2_tags[94]) #sample\n",
    "\n",
    "#renaming things for clarity:\n",
    "s1_both = s1_tagged\n",
    "s2_both = s1_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great! so now we have three objects:\n",
    "\n",
    "s1_tokens/s2_tokens: a list of the words in the sentences\n",
    "s1_tags/s2_tags: a list of the tags in the sentences\n",
    "s1_both/s2_both: a list of both the words and the tags in the sentences (x,y)\n",
    "^see why I renamed this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets store these objects into a new, less cluttered DF.\n",
    "what are some of the columns in Stimuli.xlsx that we might find useful?\n",
    "character count, WC_S1, WC_S2, S1, S2, tokenized S1, tokenized S2, tagged S1, tagged S2, tags-only S1, tags-only S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_pd = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
